{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract data from csv file and then serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les.pickle        solution.ipynb    \u001b[31mstore_states.csv\u001b[m\u001b[m* train.pickle\r\n",
      "\u001b[31mmodels.py\u001b[m\u001b[m*        store.csv         test.csv          train_XY.pickle\r\n",
      "models.pyc        store.pickle      train.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csvToDict(csvfile):\n",
    "    datas = []\n",
    "    keys = []\n",
    "    for row_index, row in enumerate(csvfile):\n",
    "        if row_index == 0:\n",
    "            keys = row\n",
    "            print keys\n",
    "            continue\n",
    "        datas.append({key : val for key, val in zip(keys, row)}) # each element is a sample in a dict format\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setNanAsString(datas, default_str = '0'):\n",
    "    for i, sample in enumerate(datas):\n",
    "        for key, val in sample.items():\n",
    "            if val == '':\n",
    "                sample[key] = default_str\n",
    "        datas[i] = sample   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = \"train.csv\"\n",
    "store_data = \"store.csv\"\n",
    "store_states_data = \"store_states.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']\n",
      "[{'DayOfWeek': '2', 'Customers': '0', 'StateHoliday': 'a', 'Promo': '0', 'Sales': '0', 'Date': '2013-01-01', 'Open': '0', 'SchoolHoliday': '1', 'Store': '1115'}, {'DayOfWeek': '2', 'Customers': '0', 'StateHoliday': 'a', 'Promo': '0', 'Sales': '0', 'Date': '2013-01-01', 'Open': '0', 'SchoolHoliday': '1', 'Store': '1114'}, {'DayOfWeek': '2', 'Customers': '0', 'StateHoliday': 'a', 'Promo': '0', 'Sales': '0', 'Date': '2013-01-01', 'Open': '0', 'SchoolHoliday': '1', 'Store': '1113'}]\n"
     ]
    }
   ],
   "source": [
    "with open(train_data) as csvfile:\n",
    "    csv_data = csv.reader(csvfile, delimiter = ',')\n",
    "    with open(\"train.pickle\",\"wb\") as dump_file:\n",
    "        datas = csvToDict(csv_data)\n",
    "        datas = datas[::-1]\n",
    "        pickle.dump(datas, dump_file, -1)\n",
    "        print datas[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
      "['Store', 'State']\n",
      "[{'CompetitionOpenSinceMonth': '9', 'Promo2SinceYear': '0', 'Promo2SinceWeek': '0', 'CompetitionDistance': '1270', 'PromoInterval': '0', 'State': 'HE', 'Promo2': '0', 'CompetitionOpenSinceYear': '2008', 'Assortment': 'a', 'StoreType': 'c', 'Store': '1'}, {'CompetitionOpenSinceMonth': '11', 'Promo2SinceYear': '2010', 'Promo2SinceWeek': '13', 'CompetitionDistance': '570', 'PromoInterval': 'Jan,Apr,Jul,Oct', 'State': 'TH', 'Promo2': '1', 'CompetitionOpenSinceYear': '2007', 'Assortment': 'a', 'StoreType': 'a', 'Store': '2'}]\n"
     ]
    }
   ],
   "source": [
    "with open(store_data) as csvfile1, open(store_states_data) as csvfile2:\n",
    "    csv_data1 = csv.reader(csvfile1, delimiter = ',')\n",
    "    csv_data2 = csv.reader(csvfile2, delimiter = ',')\n",
    "    with open(\"store.pickle\",\"wb\") as dump_file:\n",
    "        store_datas = csvToDict(csv_data1)\n",
    "        store_states_datas = csvToDict(csv_data2)\n",
    "        setNanAsString(store_datas)\n",
    "        for i, store in enumerate(store_datas):\n",
    "            store[\"State\"] = store_states_datas[i][\"State\"]\n",
    "            store_datas[i] = store\n",
    "        pickle.dump(store_datas, dump_file, -1)\n",
    "        print store_datas[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"train.pickle\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(\"store.pickle\") as f:\n",
    "    store_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCleanFeature(sample):\n",
    "    dt = datetime.strptime(sample[\"Date\"], \"%Y-%m-%d\")\n",
    "    store_index = int(sample[\"Store\"])\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    day = dt.day\n",
    "    day_of_week = int(sample[\"DayOfWeek\"])\n",
    "    try:\n",
    "        store_open = int(sample['Open'])\n",
    "    except:\n",
    "        store_open = 1\n",
    "    promo = int(sample['Promo'])    \n",
    "    return [    \n",
    "                store_open,\n",
    "                store_index,\n",
    "                day_of_week,\n",
    "                promo,\n",
    "                year,\n",
    "                month,\n",
    "                day,\n",
    "                store_data[store_index - 1]['State']\n",
    "            ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train datapoints: 4221690\n",
      "sales turnover range from: 46 to 41551\n"
     ]
    }
   ],
   "source": [
    "for sample in train_data:\n",
    "    if sample[\"Sales\"] != \"0\" and sample[\"Open\"] != \"\":\n",
    "        x = getCleanFeature(sample)\n",
    "        train_X.append(x)\n",
    "        train_Y.append(int(sample[\"Sales\"]))\n",
    "print \"number of train datapoints: \" + str(len(train_Y))\n",
    "print \"sales turnover range from: \" + str(min(train_Y)) + \" to \" + str(max(train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  0, 109,   1,   0,   0,   0,   0,   7]), 5961)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)\n",
    "full_X = np.array(train_X)\n",
    "\n",
    "les = []\n",
    "for col in range(train_X.shape[1]):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(full_X[:, col])\n",
    "    les.append(le)\n",
    "    train_X[:, col] = le.transform(train_X[:, col])\n",
    "train_X = train_X.astype(int)\n",
    "    \n",
    "with open('les.pickle', 'wb') as f:\n",
    "    pickle.dump(les, f, -1)\n",
    "\n",
    "with open('train_XY.pickle', 'wb') as f:\n",
    "    pickle.dump((train_X, train_Y), f, -1)\n",
    "    print(train_X[0], train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Merge, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "numpy.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_features(X):\n",
    "    X_list = []\n",
    "\n",
    "    store_index = X[..., [1]]\n",
    "    X_list.append(store_index)\n",
    "\n",
    "    day_of_week = X[..., [2]]\n",
    "    X_list.append(day_of_week)\n",
    "\n",
    "    promo = X[..., [3]]\n",
    "    X_list.append(promo)\n",
    "\n",
    "    year = X[..., [4]]\n",
    "    X_list.append(year)\n",
    "\n",
    "    month = X[..., [5]]\n",
    "    X_list.append(month)\n",
    "\n",
    "    day = X[..., [6]]\n",
    "    X_list.append(day)\n",
    "\n",
    "    State = X[..., [7]]\n",
    "    X_list.append(State)\n",
    "\n",
    "    return X_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN_with_EntityEmbedding(object):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        self.nb_epoch = 1\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "    \n",
    "    def preprocessing(self, X):\n",
    "        X_list = split_features(X)\n",
    "        return X_list    \n",
    "    \n",
    "    def __build_keras_model(self):\n",
    "        models = []\n",
    "\n",
    "        model_store = Sequential()\n",
    "        model_store.add(Embedding(1115, 10, input_length=1))\n",
    "        model_store.add(Reshape(target_shape=(10,)))\n",
    "        models.append(model_store)\n",
    "\n",
    "        model_dow = Sequential()\n",
    "        model_dow.add(Embedding(7, 6, input_length=1))\n",
    "        model_dow.add(Reshape(target_shape=(6,)))\n",
    "        models.append(model_dow)\n",
    "\n",
    "        model_promo = Sequential()\n",
    "        model_promo.add(Dense(1, input_dim=1))\n",
    "        models.append(model_promo)\n",
    "\n",
    "        model_year = Sequential()\n",
    "        model_year.add(Embedding(3, 2, input_length=1))\n",
    "        model_year.add(Reshape(target_shape=(2,)))\n",
    "        models.append(model_year)\n",
    "\n",
    "        model_month = Sequential()\n",
    "        model_month.add(Embedding(12, 6, input_length=1))\n",
    "        model_month.add(Reshape(target_shape=(6,)))\n",
    "        models.append(model_month)\n",
    "\n",
    "        model_day = Sequential()\n",
    "        model_day.add(Embedding(31, 10, input_length=1))\n",
    "        model_day.add(Reshape(target_shape=(10,)))\n",
    "        models.append(model_day)\n",
    "\n",
    "        model_germanstate = Sequential()\n",
    "        model_germanstate.add(Embedding(12, 6, input_length=1))\n",
    "        model_germanstate.add(Reshape(target_shape=(6,)))\n",
    "        models.append(model_germanstate)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Merge(models, mode='concat'))\n",
    "        self.model.add(Dense(1000, init='uniform'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(500, init='uniform'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(self.preprocessing(X_train), self._val_for_fit(y_train),\n",
    "                       validation_data=(self.preprocessing(X_val), self._val_for_fit(y_val)),\n",
    "                       nb_epoch=self.nb_epoch, batch_size=128,\n",
    "                       )\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        features = self.preprocessing(features)\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)\n",
    "    \n",
    "    def evaluate(self, X_val, y_val):\n",
    "        assert(min(y_val) > 0)\n",
    "        guessed_sales = self.guess(X_val)\n",
    "        relative_err = numpy.absolute((y_val - guessed_sales) / y_val)\n",
    "        result = numpy.sum(relative_err) / len(y_val)\n",
    "        return result    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "from models import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "numpy.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "shuffle_data = False\n",
    "one_hot_as_input = False\n",
    "embeddings_as_input = False\n",
    "save_embeddings = True\n",
    "save_models = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X, y) = pickle.load(open(\"train_XY.pickle\", \"rb\"))\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if shuffle_data:\n",
    "    print(\"using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if one_hot_as_input:\n",
    "    print(\"using one-hot encoding as input\")\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(X, y, n):\n",
    "    num_records = X.shape[0]\n",
    "    indices = numpy.random.randint(num_records, size=n)\n",
    "    return X[indices, :], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"number of samples used for training: \" + str(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting NN_with_EntityEmbedding...\n",
      "Train on 200000 samples, validate on 422169 samples\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 73s - loss: 0.0142 - val_loss: 0.0098\n",
      "('Result on validation data: ', 0.10834435437077393)\n"
     ]
    }
   ],
   "source": [
    "print(\"fitting NN_with_EntityEmbedding...\")\n",
    "NNEE = NN_with_EntityEmbedding(X_train, y_train, X_val, y_val)\n",
    "NNEE.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = []\n",
    "merged_layer = NNEE.model.layers[0]\n",
    "for embedding_layer in merged_layer.layers:\n",
    "    weights.append(embedding_layer.get_weights()[0])\n",
    "with open(saved_embeddings_fname, 'wb') as f:\n",
    "    pickle.dump(weights, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if save_models:\n",
    "    with open('models.pickle', 'wb') as f:\n",
    "        pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y):\n",
    "    assert(min(y) > 0)\n",
    "    guessed_sales = numpy.array([model.guess(X) for model in models])\n",
    "    mean_sales = guessed_sales.mean(axis=0)\n",
    "    relative_err = numpy.absolute((y - mean_sales) / y)\n",
    "    result = numpy.sum(relative_err) / len(y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "0.0782901170852\n",
      "Validation error...\n",
      "0.0808420976915\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
